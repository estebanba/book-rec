{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "from thefuzz import fuzz, process\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/Building_Permits__Addition_Alteration_20250305.csv\")\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")  #Convert colummns to lowercase\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"building_construction_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"energy_compliance_option\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description_of_demolition\"] = df[\"description_of_demolition\"].fillna(\"No Description\")\n",
    "df[\"description_of_demolition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"debris_disposal\"] = df[\"debris_disposal\"].fillna(\"Other\")\n",
    "df[\"debris_disposal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"method_of_removal\"] = df[\"method_of_removal\"].fillna(\"Other\")\n",
    "df[\"method_of_removal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"number_of_units\"] = df[\"number_of_units\"].fillna(0)\n",
    "df[\"number_of_units\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"number_of_units\"].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"current_property_use\"] = df[\"current_property_use\"].fillna(\"Not Specified\")\n",
    "df[\"current_property_use\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"building_use\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"size_of_new_addition\"] = df[\"size_of_new_addition\"].fillna(0)\n",
    "df[\"size_of_new_addition\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"firm_name\"] = df[\"firm_name\"].fillna(\"Other\")\n",
    "df[\"firm_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"isd_description\"] = df[\"isd_description\"].fillna(\"Not Specified\")\n",
    "df[\"isd_description\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=True).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop selected Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[ \"address_for_mapping\", \"submit_date\", \"change_in_units\", \"change_in_property_use\", \"maplot_number\", \"id_field\", \"viewpoint_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all Columns where NaN values are above the given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaN values in each column\n",
    "nan_counts = df.isna().sum()\n",
    "\n",
    "# Sort columns by NaN counts (ascending)\n",
    "sorted_columns = nan_counts.sort_values().index.tolist()\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"\\nNaN counts per column:\")\n",
    "print(nan_counts)\n",
    "print(\"\\nColumns sorted by NaN counts (ascending):\")\n",
    "print(sorted_columns)\n",
    "\n",
    "# Create a new DataFrame with columns sorted by NaN counts\n",
    "sorted_df = df[sorted_columns]\n",
    "\n",
    "# Filter columns based on a threshold (e.g., keep only columns with fewer than 2 NaNs)\n",
    "threshold = 5000\n",
    "filtered_columns = nan_counts[nan_counts < threshold].index.tolist()\n",
    "df = df[filtered_columns]\n",
    "print(\"\\nShape of DataFrame with columns having fewer than\", threshold, \"NaNs:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast date column as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_date'] = pd.to_datetime(df['issue_date'], errors='coerce')\n",
    "df['issue_year'] = df['issue_date'].dt.year\n",
    "df['issue_month'] = df['issue_date'].dt.month\n",
    "print(\"\\nDate range of permits:\")\n",
    "print(f\"From {df['issue_date'].min()} to {df['issue_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        else:\n",
    "            return 'Autumn'\n",
    "        \n",
    "df['season'] = df['issue_month'].apply(get_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing True/False and fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"condo_association\"] = df[\"condo_association\"].fillna(False)\n",
    "# df[\"condo_association\"] = df[\"condo_association\"].apply(lambda x: int(x))\n",
    "df[\"condo_association\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bza_case\"] = df[\"bza_case\"].fillna(False)\n",
    "# df[\"bza_case\"] = df[\"bza_case\"].apply(lambda x: int(x))\n",
    "df[\"bza_case\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"planning_board_special_permit\"] = df[\"planning_board_special_permit\"].fillna(False)\n",
    "# df[\"planning_board_special_permit\"] = df[\"planning_board_special_permit\"].apply(lambda x: int(x))\n",
    "df[\"planning_board_special_permit\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bicycle_parking_change\"] = df[\"bicycle_parking_change\"].apply(lambda x: False if x == \"No\" else True)\n",
    "df[\"bicycle_parking_change\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"status\"] = df[\"status\"].apply(lambda x: int(0) if x == \"Active\" else int(1))\n",
    "df[\"status\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop remaining rows with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create calculated columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"calc_total_cost\"] = (df[\"building_cost\"] + df[\"electrical_cost\"] + df['plumbing_cost'] + df['gas_cost'] + df['hvac_cost'] + df['fire_prevention_cost'])\n",
    "df[[\"total_cost\", \"calc_total_cost\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original total_cost\n",
    "df.drop(\"total_cost\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(df, column):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\n",
    "    sns.boxplot(x = df[column], color=\"lightblue\", ax=ax1)\n",
    "    sns.histplot(df[column], kde=True, bins=30, color=\"salmon\", ax=ax2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df_test_total_cost = df[df[\"calc_total_cost\"] < 10000000]\n",
    "df_test_number_of_units = df[(df[\"number_of_units\"] < 10)]\n",
    "\n",
    "testing_dict = {\n",
    "    \"calc_total_cost\": df_test_total_cost, \n",
    "    \"number_of_units\": df_test_number_of_units\n",
    "}\n",
    "\n",
    "for key, value in testing_dict.items():\n",
    "    plot_outliers(value, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(df[df[\"electrical_cost\"] > 1000000], \"electrical_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(df[df[\"fire_prevention_cost\"] < 250000], \"fire_prevention_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"fire_prevention_cost\"] < 250000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(df[df[\"hvac_cost\"] < 250000], \"hvac_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretizing Features to include Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculated Total Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing 'total_cost' into 3 categories\n",
    "bins = [0, 20000, 200000, 2000000, df['calc_total_cost'].max()]\n",
    "labels = [\"low\", \"medium\", \"high\", \"very high\"]\n",
    "df['total_cost_bins'] = pd.cut(df['calc_total_cost'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "df['total_cost_bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"calc_total_cost\"] < 10000000]\n",
    "df = df[df[\"number_of_units\"] < 200]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop One/Two Family dwellings where there are more than 1 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[(df[\"number_of_units\"] > 2.0) & (df[\"building_use\"] == \"One or Two Family Dwelling\")].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"number_of_units\"] > 2.0) & (df[\"building_use\"] == \"One or Two Family Dwelling\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deleting edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"record_number\"] == 2323]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df[\"record_number\"] == 3877].index)\n",
    "df = df.drop(df[df[\"record_number\"] == 8409].index)\n",
    "df = df.drop(df[df[\"record_number\"] == 2304].index)\n",
    "df = df.drop(df[df[\"record_number\"] == 2323].index)\n",
    "df = df.drop(df[df[\"record_number\"] == 106].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Firma Name's Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(list(df[\"firm_name\"].unique()))\n",
    "df[\"firm_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firm_name_cleaner import clean_firm_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_firm_names(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(list(df[\"standardized_firm_name\"].unique()))\n",
    "df[\"standardized_firm_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[['original_firm_name', 'firm_name', 'standardized_firm_name']].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Description into a soup of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text before keyword extraction\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Handle NaN values\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Fix spacing\n",
    "    text = re.sub(r'\\s+', ' ', str(text).strip())\n",
    "    \n",
    "    # Fix common typos\n",
    "    text = text.replace(\"wityh\", \"with\")\n",
    "    \n",
    "    # Remove parentheses and other problematic characters\n",
    "    text = re.sub(r'[\\(\\)]', ' ', text)\n",
    "    \n",
    "    # Clean up numbers followed by single characters\n",
    "    text = re.sub(r'(\\d+)\\s+([a-zA-Z])\\s', r'\\1\\2 ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_keywords_with_context(text):\n",
    "    \"\"\"\n",
    "    Extract meaningful keyword phrases from construction/repair text using spaCy\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        list: List of extracted meaningful keyword phrases\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    text = clean_text(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    keywords = []\n",
    "    \n",
    "    # Extract verb phrases with their objects (install wall, replace floor)\n",
    "    keywords.extend(extract_construction_verb_phrases(doc))\n",
    "    \n",
    "    # Extract material + component combinations (metal stud, wood floor)\n",
    "    keywords.extend(extract_material_component_combinations(doc))\n",
    "    \n",
    "    # Extract measurements with context (5/8 fire, 3 kw)\n",
    "    keywords.extend(extract_measurements_with_context(doc))\n",
    "    \n",
    "    # Extract important noun phrases (kitchen floor, hood vent)\n",
    "    keywords.extend(extract_noun_phrases(doc))\n",
    "    \n",
    "    # Extract important compound terms (noun+noun combinations)\n",
    "    keywords.extend(extract_compound_terms(doc))\n",
    "    \n",
    "    # Final cleanup and filtering\n",
    "    final_keywords = filter_and_clean_keywords(keywords)\n",
    "    \n",
    "    return final_keywords\n",
    "\n",
    "def extract_construction_verb_phrases(doc):\n",
    "    \"\"\"Extract verb phrases related to construction/repair\"\"\"\n",
    "    keywords = []\n",
    "    construction_verbs = ['install', 'replace', 'repair', 'remove', 'build', 'construct']\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in construction_verbs:\n",
    "            # Find all noun phrases that are children of this verb\n",
    "            obj_tokens = []\n",
    "            for child in token.children:\n",
    "                if child.pos_ in ['NOUN', 'PROPN']:\n",
    "                    # Include modifiers of this noun\n",
    "                    modifiers = []\n",
    "                    for modifier in child.children:\n",
    "                        if modifier.pos_ in ['ADJ', 'NOUN', 'PROPN'] or modifier.dep_ == 'compound':\n",
    "                            modifiers.append(modifier.text.lower())\n",
    "                    \n",
    "                    # Create phrase with modifiers + noun\n",
    "                    if modifiers:\n",
    "                        phrase = ' '.join(modifiers) + ' ' + child.text.lower()\n",
    "                    else:\n",
    "                        phrase = child.text.lower()\n",
    "                    \n",
    "                    obj_tokens.append(phrase)\n",
    "            \n",
    "            # Create construction action phrases\n",
    "            for obj in obj_tokens:\n",
    "                action_phrase = f\"{token.lemma_.lower()} {obj}\"\n",
    "                if action_phrase not in keywords:\n",
    "                    keywords.append(action_phrase)\n",
    "                    \n",
    "    return keywords\n",
    "\n",
    "def extract_material_component_combinations(doc):\n",
    "    \"\"\"Extract material + component combinations\"\"\"\n",
    "    keywords = []\n",
    "    material_words = ['metal', 'wood', 'vinyl', 'plastic', 'steel', 'aluminum', 'fire']\n",
    "    component_words = ['stud', 'wall', 'floor', 'ceiling', 'door', 'window', 'vent', 'hood', 'cabinet', 'heat']\n",
    "    \n",
    "    # Find material + component combinations\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.lemma_.lower() in material_words:\n",
    "            # Look ahead for component words\n",
    "            for j in range(i+1, min(i+4, len(doc))):\n",
    "                if doc[j].lemma_.lower() in component_words:\n",
    "                    material_component = f\"{token.lemma_.lower()} {doc[j].lemma_.lower()}\"\n",
    "                    if material_component not in keywords:\n",
    "                        keywords.append(material_component)\n",
    "                        \n",
    "    return keywords\n",
    "\n",
    "def extract_measurements_with_context(doc):\n",
    "    \"\"\"Extract measurements with their context\"\"\"\n",
    "    keywords = []\n",
    "    measurement_pattern = re.compile(r'\\d+/\\d+|\\d+\\.\\d+|\\d+')\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        if measurement_pattern.match(token.text):\n",
    "            # Check if next token is a unit or material\n",
    "            if i < len(doc) - 1:\n",
    "                # Skip single characters or stopwords as the next token\n",
    "                if len(doc[i+1].text) <= 1 or doc[i+1].is_stop:\n",
    "                    continue\n",
    "                \n",
    "                measurement = f\"{token.text} {doc[i+1].text.lower()}\"\n",
    "                if measurement not in keywords:\n",
    "                    keywords.append(measurement)\n",
    "                    \n",
    "    return keywords\n",
    "\n",
    "def extract_noun_phrases(doc):\n",
    "    \"\"\"Extract important noun phrases\"\"\"\n",
    "    keywords = []\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        # Skip chunks with only stopwords\n",
    "        if all(token.is_stop for token in chunk):\n",
    "            continue\n",
    "            \n",
    "        # Extract clean noun phrases\n",
    "        if 2 <= len(chunk) <= 3:\n",
    "            clean_phrase = ' '.join([token.lemma_.lower() for token in chunk \n",
    "                                    if not token.is_punct and len(token.text) > 1 \n",
    "                                    and not token.is_stop])\n",
    "            if clean_phrase and len(clean_phrase) > 2 and clean_phrase not in keywords:\n",
    "                keywords.append(clean_phrase)\n",
    "                \n",
    "    return keywords\n",
    "\n",
    "def extract_compound_terms(doc):\n",
    "    \"\"\"Extract compound terms (noun+noun combinations)\"\"\"\n",
    "    keywords = []\n",
    "    \n",
    "    for i in range(len(doc) - 1):\n",
    "        if doc[i].pos_ in ['NOUN', 'PROPN'] and doc[i+1].pos_ in ['NOUN', 'PROPN']:\n",
    "            if not doc[i].is_stop and not doc[i+1].is_stop:\n",
    "                compound = f\"{doc[i].lemma_.lower()} {doc[i+1].lemma_.lower()}\"\n",
    "                if compound not in keywords:\n",
    "                    keywords.append(compound)\n",
    "                    \n",
    "    return keywords\n",
    "\n",
    "def filter_and_clean_keywords(keywords):\n",
    "    \"\"\"Final filtering and cleaning of keywords\"\"\"\n",
    "    final_keywords = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        # Skip single words that are too short\n",
    "        words = keyword.split()\n",
    "        if len(words) == 1 and len(keyword) <= 2:\n",
    "            continue\n",
    "            \n",
    "        # Skip phrases with single characters or parentheses\n",
    "        if re.search(r'\\b[a-zA-Z]\\b|\\(|\\)', keyword):\n",
    "            continue\n",
    "            \n",
    "        # Skip phrases with only numbers and single characters\n",
    "        if re.match(r'^\\d+\\s+[a-zA-Z]$', keyword):\n",
    "            continue\n",
    "            \n",
    "        # Check if this keyword is a subset of an existing keyword\n",
    "        if not any(keyword in k and keyword != k for k in keywords):\n",
    "            final_keywords.append(keyword)\n",
    "    \n",
    "    return final_keywords\n",
    "\n",
    "def process_dataframe(df, description_col='description', id_col=None):\n",
    "    \"\"\"\n",
    "    Process a dataframe to extract keywords from a description column\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input dataframe\n",
    "        description_col (str): Name of the description column\n",
    "        id_col (str, optional): Name of the ID column. If None, uses dataframe index.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added keywords column\n",
    "    \"\"\"\n",
    "    # Check if the description column exists\n",
    "    if description_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{description_col}' not found in dataframe\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Add record_number column if id_col is not specified\n",
    "    if id_col is None:\n",
    "        result_df['record_number'] = result_df.index\n",
    "    else:\n",
    "        if id_col not in df.columns:\n",
    "            raise ValueError(f\"ID column '{id_col}' not found in dataframe\")\n",
    "        result_df['record_number'] = result_df[id_col]\n",
    "    \n",
    "    # Extract keywords for each row with progress bar\n",
    "    print(f\"Extracting keywords from '{description_col}' column...\")\n",
    "    result_df['keywords'] = [extract_keywords_with_context(text) for text in tqdm(result_df[description_col].tolist())]\n",
    "    \n",
    "    print(f\"Keyword extraction complete. Added 'keywords' column.\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_dataframe(df, description_col='description')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keyword_dataframe(df_with_keywords):\n",
    "    \"\"\"\n",
    "    Create a new dataframe where each keyword is in a separate row\n",
    "    with the record_number from original data\n",
    "    \"\"\"\n",
    "    keyword_records = []\n",
    "    \n",
    "    # Make sure you're using the EXACT same record_number from the original dataframe\n",
    "    for _, row in df_with_keywords.iterrows():\n",
    "        record_number = row['record_number']  # This must be the original ID\n",
    "        \n",
    "        for keyword in row['keywords']:\n",
    "            keyword_records.append({\n",
    "                'record_number': record_number,  # Use the original record_number\n",
    "                'keyword': keyword\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(keyword_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df = create_keyword_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the keyword dataframe\n",
    "print(\"\\nKeyword dataframe sample:\")\n",
    "display(keyword_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df[df[\"record_number\"] == 11875][[\"description\", \"keywords\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(keyword_df.tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df[\"keyword\"].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add frequency count to keywords\n",
    "keyword_counts = keyword_df['keyword'].value_counts().reset_index()\n",
    "keyword_counts.columns = ['keyword', 'frequency']\n",
    "\n",
    "# Merge to add frequency to each keyword instance\n",
    "keyword_df_with_freq = pd.merge(keyword_df, keyword_counts, on='keyword', how='left')\n",
    "\n",
    "# Export keyword dataframe\n",
    "keyword_df_with_freq.to_csv(\"../data/clean/keyword_data.csv\", index=False)\n",
    "print(f\"Keyword dataframe exported with {len(keyword_df_with_freq)} rows.\")\n",
    "\n",
    "# 3. Create a summary file with unique keywords and their frequencies for the word cloud\n",
    "unique_keywords = keyword_counts.copy()\n",
    "unique_keywords.to_csv(\"../data/clean/unique_keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalize Current Property Use and Building Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df[\"current_property_use\"] != 'Vacant Lot') & (df[\"current_property_use\"] != 'Accessory')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"current_property_use\"].unique())\n",
    "print(df[\"building_use\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_use_dict = {\n",
    "    \"Not Specified\": \"Commercial/Mixed\",\n",
    "    \"Commercial / Mixed Use\": \"Commercial/Mixed\",\n",
    "\n",
    "    \"Multi-Family (3 units or greater)\": \"Multi-Family\",\n",
    "    \"Multi Family (3 or more dwelling units)\": \"Multi-Family\",\n",
    "\n",
    "    'Two-Family': 'One/Two-Family',\n",
    "    'One-Family': 'One/Two-Family',\n",
    "    'One or Two Family Dwelling': 'One/Two-Family',\n",
    "\n",
    "    'Townhouse': 'Townhouse',\n",
    "}\n",
    "\n",
    "df = df.replace({\"current_property_use\": building_use_dict})\n",
    "df = df.replace({\"building_use\": building_use_dict})\n",
    "\n",
    "df[[\"current_property_use\", \"building_use\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "\n",
    "# for column in columns:\n",
    "#     if df[column].nunique() == 2:\n",
    "#         print(column, df[column].unique(), df[column].dtype)\n",
    "\n",
    "binary_columns = {}\n",
    "for column in df.columns:\n",
    "        try:\n",
    "            # Check if column has exactly 2 unique values\n",
    "            unique_values = df[column].dropna().unique()\n",
    "            \n",
    "            # Only consider a column binary if it has exactly 2 unique values\n",
    "            if len(unique_values) == 2:\n",
    "                binary_columns[column] = list(unique_values)\n",
    "                print(f\"Binary column found: {column}, Values: {unique_values}, Type: {df[column].dtype}\")\n",
    "        except TypeError as e:\n",
    "            # Handle unhashable types (like lists or dicts in cells)\n",
    "            print(f\"Skipping column '{column}' - contains unhashable type: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast string booleans as booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = [\n",
    "    \"change_in_exterior\", \n",
    "    \"discharge_to_sewer_or_storm_water_system\", \n",
    "    \"new_or_replaced_storm_sewer\", \n",
    "    \"construction_dewatering\", \n",
    "    \"public_right-of-way\", \n",
    "    \"basement_plumbing_fixture\", \n",
    "    \"change_in_at_least_half_of_total_area\",\n",
    "]\n",
    "\n",
    "df[boolean_columns] = df[boolean_columns].replace({\"True\": True, \"False\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"record_number\"] == 3877]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving clean DataFrame as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/clean/building_permits_addition_alteration_clean.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
